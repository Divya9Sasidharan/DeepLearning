{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_7__Assorted_Programming_Puzzles_(1) (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f2Gtdy80b5Al","colab_type":"text"},"source":["# Assignment 7 submission: Assorted Programming Puzzles\n","\n","## Group members: Divya Sasidharan, Poornima Venkatesha, Sinchana Eshwarappa Prameela"]},{"cell_type":"markdown","metadata":{"id":"6jAOT3w2aIfr","colab_type":"text"},"source":["### 1) Given a 2D tensor of shape (?, n), extract the k (k <= n) highest values for each row into a tensor of shape (?, k). Hint: There might be a function to get the “top k” values of a tensor."]},{"cell_type":"code","metadata":{"id":"He5pH8L-aN4g","colab_type":"code","outputId":"3b66ac6d-e537-4819-9d23-ca81e2984fd0","colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","# 2D Tensor (matrix)\n","tensor_2D = np.array([[50, 100, 300,299],  # row 1\n","                      [256, 356, 389,200],  # row 2\n","                      [100, 300, 200,200], #row 3\n","                      [167, 899,100,200]])  # row 4\n","print(\"input tensor shape is {}\".format(tensor_2D.shape))\n","print(\"2D Tensor:\\n{}\".format(tensor_2D))\n","print(\"Tensor dimensions: {}\".format(tensor_2D.ndim))\n","kvalue=tensor_2D.shape[0] -1\n","top_K=tf.math.top_k(tensor_2D, k=kvalue, sorted=True, name=\"topK\").values\n","print(\" new 2D Tensor of shape {} with {} highest values for each row:\\n{}\".format(top_K.shape,kvalue,top_K))\n","\n","\n","\n","print('-----with decorator------')\n","@tf.function\n","def example_1(tensor_in):\n","  print(\"input tensor shape is {}\".format(tensor_2D.shape))\n","  print(\"2D Tensor:\\n{}\".format(tensor_2D))\n","  print(\"Tensor dimensions: {}\".format(tensor_2D.ndim))\n","  kvalue=tensor_2D.shape[0] -1\n","  top_K=tf.math.top_k(tensor_2D, k=kvalue, sorted=True, name=\"topK\").values\n","  print(\" new 2D Tensor of shape {} with {} highest values for each row:\\n{}\".format(top_K.shape,kvalue,top_K))\n","  return top_K\n","example_1(tensor_2D)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["input tensor shape is (4, 4)\n","2D Tensor:\n","[[ 50 100 300 299]\n"," [256 356 389 200]\n"," [100 300 200 200]\n"," [167 899 100 200]]\n","Tensor dimensions: 2\n"," new 2D Tensor of shape (4, 3) with 3 highest values for each row:\n","[[300 299 100]\n"," [389 356 256]\n"," [300 200 200]\n"," [899 200 167]]\n","-----with decorator------\n","input tensor shape is (4, 4)\n","2D Tensor:\n","[[ 50 100 300 299]\n"," [256 356 389 200]\n"," [100 300 200 200]\n"," [167 899 100 200]]\n","Tensor dimensions: 2\n"," new 2D Tensor of shape (4, 3) with 3 highest values for each row:\n","Tensor(\"topK:0\", shape=(4, 3), dtype=int64)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n","array([[300, 299, 100],\n","       [389, 356, 256],\n","       [300, 200, 200],\n","       [899, 200, 167]])>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"4zrBY-memtlW","colab_type":"text"},"source":["### 2) Given a tensor of shape (?, n), find the argmax in each row and return a new tensor that contains a 1 in each of the argmax’ positions, and 0s everywhere else."]},{"cell_type":"code","metadata":{"id":"YmQASG_VSWvq","colab_type":"code","outputId":"df47eed0-6493-4590-a7b4-62b27636a7e2","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","# 2D Tensor (matrix)\n","tensor_2D = np.array([[50, 100, 300,2990,5],  # row 1\n","                      [256, 356, 389,200,6],  # row 2\n","                      [100, 300, 200,200,7], #row 3\n","                      [1067, 899,100,200,0]])  # row 4\n","updates_final=[]\n","for n in range(len(tensor_2D)):  \n","  updates = [0]*len(tensor_2D[n])\n","  updates[tf.math.argmax(tensor_2D[n]).numpy()]=1  \n","  updates_final.append(updates)\n","final_tensor=tf.convert_to_tensor(updates_final)\n","print(tensor_2D)\n","print(final_tensor)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[  50  100  300 2990    5]\n"," [ 256  356  389  200    6]\n"," [ 100  300  200  200    7]\n"," [1067  899  100  200    0]]\n","tf.Tensor(\n","[[0 0 0 1 0]\n"," [0 0 1 0 0]\n"," [0 1 0 0 0]\n"," [1 0 0 0 0]], shape=(4, 5), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jb--G0iTlL5Q","colab_type":"text"},"source":["### 3)As in 1., but instead of “extracting” the top k values, create a new tensor with shape (?, n) where all but the top k values for each row are zero. Try doing this with a 1D tensor of shape (n,) (i.e. one row) first. Getting it right for a 2D tensor is more tricky; consider this a bonus. Hint: You should look for a way to “scatter” a tensor of values into a different tensor. For two or more dimensions, you need to think carefully about the indices."]},{"cell_type":"code","metadata":{"id":"CXC_PAFplsGu","colab_type":"code","outputId":"f07fe02b-35f2-4e54-dea5-f1195669a56b","colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","# 1D Tensor (matrix)\n","tensor_1D = np.array([50, 100, 300,2990])\n","print(\"input tensor shape is {}\".format(tensor_1D.shape))\n","print(\"1D Tensor:\\n{}\".format(tensor_1D))\n","print(\"Tensor dimensions: {}\".format(tensor_1D.ndim))\n","kvalue=tensor_1D.shape[0] -1\n","top_K=tf.math.top_k(tensor_1D, k=kvalue, sorted=True, name=\"topK\").values\n","print(\" new 1D Tensor of shape {} with {} highest values for each row:\\n{}\".format(top_K.shape,kvalue,top_K))\n","updates = [0]*len(tensor_1D)\n","for n in (tf.math.top_k(tensor_1D, k=kvalue, sorted=True, name=\"topK\").indices):  \n","  updates[n]=tensor_1D[n]\n","print(\"final 1D tensor is :-->\")\n","final_tensor=tf.convert_to_tensor(updates)\n","print(final_tensor)\n","\n","print(\"------------------------------------------------------------------------------------------------------------------------------------\")\n","\n","# 2D Tensor (matrix)\n","tensor_2D = np.array([[50, 100, 300,2990,5],  # row 1\n","                      [256, 356, 389,200,6],  # row 2\n","                      [100, 300, 200,200,7], #row 3\n","                      [1067, 899,100,200,0]])  # row 4\n","print(\"input tensor shape is {}\".format(tensor_2D.shape))\n","print(\"2D Tensor:\\n{}\".format(tensor_2D))\n","print(\"Tensor dimensions: {}\".format(tensor_2D.ndim))\n","kvalue=tensor_2D.shape[0] -1\n","top_K=tf.math.top_k(tensor_2D, k=kvalue, sorted=True, name=\"topK\").values\n","print(\" new 2D Tensor of shape {} with {} highest values for each row:\\n{}\".format(top_K.shape,kvalue,top_K))\n","j=0\n","updates_final =[]\n","for n in (tf.math.top_k(tensor_2D, k=kvalue, sorted=True, name=\"topK\").indices):  \n","  updates = [0]*(tensor_2D.shape[1]) \n","  for i in n:    \n","    updates[i]=tensor_2D[j][i]\n","  updates_final.append(updates)  \n","  j=j+1\n","final_tensor=tf.convert_to_tensor(updates_final)\n","print(\"final 2D tensor is :-->\")\n","print(final_tensor)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["input tensor shape is (4,)\n","1D Tensor:\n","[  50  100  300 2990]\n","Tensor dimensions: 1\n"," new 1D Tensor of shape (3,) with 3 highest values for each row:\n","[2990  300  100]\n","final 1D tensor is :-->\n","tf.Tensor([   0  100  300 2990], shape=(4,), dtype=int32)\n","------------------------------------------------------------------------------------------------------------------------------------\n","input tensor shape is (4, 5)\n","2D Tensor:\n","[[  50  100  300 2990    5]\n"," [ 256  356  389  200    6]\n"," [ 100  300  200  200    7]\n"," [1067  899  100  200    0]]\n","Tensor dimensions: 2\n"," new 2D Tensor of shape (4, 3) with 3 highest values for each row:\n","[[2990  300  100]\n"," [ 389  356  256]\n"," [ 300  200  200]\n"," [1067  899  200]]\n","final 2D tensor is :-->\n","tf.Tensor(\n","[[   0  100  300 2990    0]\n"," [ 256  356  389    0    0]\n"," [   0  300  200  200    0]\n"," [1067  899    0  200    0]], shape=(4, 5), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RSEja_xhmZoW","colab_type":"text"},"source":["### 4)Implement an exponential moving average. That is, given a decay rate a and an input tensor of length T, create a new length T tensor where new[0] = input[0] and new[t] = a * new[t-1] + (1-a) * input[t] otherwise. Do not use tf.train.ExponentialMovingAverage."]},{"cell_type":"code","metadata":{"id":"EGvw4IgsrUHS","colab_type":"code","outputId":"c2ca75de-771d-4444-fcf6-b33d0210131a","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["a=1.1\n","T=10\n","inp = tf.random.uniform([T])\n","\n","def new_tensor(inp):\n","  input = inp\n","  new = np.zeros(T)\n","  new[0] = input[0]\n","  for t in range(1,T):\n","    new[t] = a * new[t-1] + (1.-a) * input[t]\n","  \n","  return tf.convert_to_tensor(new)\n","\n","print(inp)\n","\n","print(new_tensor(inp))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[0.680709   0.14049423 0.7075566  0.28323436 0.16617465 0.2521243\n"," 0.28247166 0.06483459 0.10741627 0.07048941], shape=(10,), dtype=float32)\n","tf.Tensor(\n","[0.680709   0.73473048 0.73744786 0.78286922 0.84453869 0.90378016\n"," 0.96591103 1.05601871 1.15087903 1.25891793], shape=(10,), dtype=float64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KHylmX6vm175","colab_type":"text"},"source":["### 5) Find a way to return the last element in 4. without using loops. That is, return new[T] only – you don’t need to compute the other time steps (if you can avoid it)"]},{"cell_type":"code","metadata":{"id":"xRadORRwm8-c","colab_type":"code","colab":{}},"source":["a=0.5\n","T=10\n","inp = tf.random.uniform([T])\n","\n","def new_tensor(inp):\n","  input = inp\n","\n","new_tensor(inp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDDOFgLBnFwu","colab_type":"text"},"source":["### 6)Given three integer tensors x, y, z all of the same (arbitrary) shape, create a new tensor that takes values from y where x is even and from z where x is odd."]},{"cell_type":"code","metadata":{"id":"9_nNTIFGnKIi","colab_type":"code","outputId":"241edb3f-4069-4b12-ce1f-22bd2e0e6b50","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","x=tf.constant([1, 2, 3, 4, 5, 6],dtype=tf.int64)\n","y=tf.constant([111, 222, 333, 444, 555, 666],dtype=tf.int64)\n","z=tf.constant([30000, 50000, 70000, 11000, 90000, 13000],dtype=tf.int64)\n","new=tf.Variable(tf.zeros_like(x) ,dtype=tf.int64)\n","even=tf.constant(2,dtype=tf.int64)\n","check=tf.math.floormod(x,even)\n","comparison = tf.equal( check, tf.constant( 1,dtype=tf.int64 ) )\n","new.assign( tf.where (comparison,z,y) )\n","new_tensor=tf.convert_to_tensor(new, dtype=tf.int64)\n","print(new_tensor)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor([30000   222 70000   444 90000   666], shape=(6,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nng1vuYFnQB5","colab_type":"text"},"source":["### 7) Given a tensor of arbitrary and unknown shape (but at least one dimension), return 100 if the last dimension has size > 100, 12 if the last dimension has size <= 100 and > 44, and return 0 otherwise."]},{"cell_type":"code","metadata":{"id":"Lztnb509nTTH","colab_type":"code","outputId":"400e29eb-ed13-4e7d-eb2c-0e0f15253600","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import tensorflow as tf\n","\n","def count():\n","  n= int(input(\"Enter the dimension:\"))\n","  y=tf.constant(1,shape=(n,n))\n","  dim=y.ndim\n","  last_dimension=(tf.shape(y)[dim-1])\n","  print(last_dimension)\n","  if last_dimension>100:\n","    print(100)\n","    return 100\n","  elif (last_dimension<=100 and last_dimension > 44):\n","    print(12)\n","    return 12\n","  else:\n","    print(0)\n","    return 0\n","count()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Enter the dimension:300\n","tf.Tensor(300, shape=(), dtype=int32)\n","100\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"HvzhUWyanUHv","colab_type":"text"},"source":["### 8) As 7., but also create three global counts (integers), where count i should grow by 1 if condition i happened. Run the function from 7. multiple times to test whether your counting works. Now, add a @tf.function decorator to the function from 7. Does your counter still work? If not, why not? Can you change it so it does work?"]},{"cell_type":"code","metadata":{"id":"jwwCgginnaEy","colab_type":"code","outputId":"41de7651-6187-419b-95c4-334b960e7cca","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["import tensorflow as tf\n","x=[0,0,0]\n","\n","\n","def count(n):  \n","  y=tf.constant(1,shape=(n,n))\n","  dim=y.ndim  \n","  last_dimension=(tf.shape(y)[dim-1])  \n","  if last_dimension>100:    \n","    return 0\n","  elif (last_dimension<=100 and last_dimension > 44):    \n","    return 1\n","  else:    \n","    return 2\n","\n","input=[19,30,45,99,200,300,400]\n","for n in input:\n","  i=count(n)\n","  x[i]=x[i]+1\n","print(x)\n","print(\"-------------------------------------------------------------\")\n","\n","\n","\n","test=[0,0,0]\n","\n","\n","def count2(n):  \n","  y=tf.constant(1,shape=(n,n))  \n","  dim=2\n","  last_dimension=(tf.shape(y)[dim-1])  \n","  if last_dimension>100:    \n","    test[0]= test[0]+1  \n","    return 0\n","  elif (last_dimension<=100 and last_dimension > 44):\n","    test[1]= test[1]+1    \n","    return 1\n","  else: \n","    test[2]= test[2]+1   \n","    return 2\n","\n","input=[19,30,45,99,200,300,400]\n","for n in input:\n","  count2(n)  \n","print(test)\n","\n","\n","print(\"-------------------------------------------------------------\")\n","print('------moved the counter outside the function to resolve the error---- ')\n","z=[0,0,0]\n","@tf.function\n","def count3(n):  \n","  y=tf.constant(1,shape=(n,n))  \n","  dim=2\n","  last_dimension=(tf.shape(y)[dim-1])  \n","  if last_dimension>100:     \n","    return 0\n","  elif (last_dimension<=100 and last_dimension > 44):       \n","    return 1\n","  else:     \n","    return 2\n","\n","input=[19,30,45,99,200,300,400]\n","for n in input:\n","  i=count3(n)\n","  z[i]=z[i]+1  \n","print(z)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[3, 2, 2]\n","-------------------------------------------------------------\n","[3, 2, 2]\n","-------------------------------------------------------------\n","------moved the counter outside the function to resolve the error---- \n","WARNING:tensorflow:5 out of the last 5 calls to <function count3 at 0x7fd326e85ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function count3 at 0x7fd326e85ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","WARNING:tensorflow:7 out of the last 7 calls to <function count3 at 0x7fd326e85ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n","[3, 2, 2]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qh8BZ4FLnc3U","colab_type":"text"},"source":["### 9) Given two 1D tensors of equal length n, create a tensor of shape (n, n) where element i,j is the ith element of the first tensor minus the jth element of the second tensor. No loops! Hint: Tensorflow supports broadcasting much like numpy."]},{"cell_type":"code","metadata":{"id":"9YqqCtlWnikl","colab_type":"code","outputId":"9588d30b-610d-4587-e97a-ad5cbdb12a11","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["import tensorflow as tf\n","\n","x = tf.constant([1, 2,3])\n","y = tf.constant([3, 4,3])\n","if len(x)==len(y):\n","  n=len(x)\n","  x_new=tf.reshape(x,shape=(1,n))\n","  x2 = tf.broadcast_to(tf.transpose(x_new), [n, n])\n","  y = tf.broadcast_to(y, [n, n])\n","  new=tf.subtract(x2,y)\n","  print(new)\n","else:\n","  print('ERROR: length miss match')\n","\n","print('----with decorator------')\n","# with decorartor\n","@tf.function\n","def example():\n","  x = tf.constant([1, 2,3])\n","  y = tf.constant([3, 4,3])\n","  if len(x)==len(y):\n","    n=len(x)\n","    x_new=tf.reshape(x,shape=(1,n))\n","    x2 = tf.broadcast_to(tf.transpose(x_new), [n, n])\n","    y = tf.broadcast_to(y, [n, n])\n","    new=tf.subtract(x2,y) \n","    return new   \n","  else:\n","    return 'ERROR: length miss match'\n","  \n","example()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[-2 -3 -2]\n"," [-1 -2 -1]\n"," [ 0 -1  0]], shape=(3, 3), dtype=int32)\n","----with decorator------\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n","array([[-2, -3, -2],\n","       [-1, -2, -1],\n","       [ 0, -1,  0]], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"IMrVl_EOnlk0","colab_type":"text"},"source":["### 10) Implement dot product attention: You are given a sequence of encoder states h of shape batch x time x features and the last decoder state s of shape batch x features. Compute the attention weights alpha where alpha[:, i] is equal to h[:, i] * s where * is the dot product between vectors (in this case we also have a batch dimension so the dot product should be between the corresponding vectors within the batch). That is, alpha should be of shape batch x time and alpha[:, i] should contain the attention weights for encoder time step i."]},{"cell_type":"code","metadata":{"id":"9igDx0zOfyZp","colab_type":"code","outputId":"2bd88a53-af8b-459e-81ab-cf2df00110d5","colab":{"base_uri":"https://localhost:8080/","height":663}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","batch=10\n","time=5\n","features=3\n","\n","\n","initializer = tf.keras.initializers.GlorotUniform\n","h=tf.Variable(initializer()([batch,time,features]))\n","s=tf.Variable(initializer()([batch,features]))\n","s=tf.transpose(s)\n","alpha=tf.Variable(tf.zeros([batch,time]))\n","for time_step in tf.range(tf.shape(alpha)[1]):        \n","    m=tf.matmul(h[:,time_step,:],s) \n","    sum= tf.reduce_sum(m,1)\n","    alpha[:,time_step].assign(sum) \n","    probability=tf.nn.softmax(alpha).numpy()[0]    \n","    p=0\n","    for i in probability:\n","      p=p+i          \n","    print(\"Attention weights alpha for encoder time step {}: \\n {}\".format(time_step,alpha[:,time_step]))\n","    print(\"Total Probability for attention weight alpha at time step {} is {} \".format(time_step ,round(p,4)))\n","    print(\"-------------------------------------------------------------------------------------------------\")\n","print('Final alpha is :')\n","print(alpha)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Attention weights alpha for encoder time step 0: \n"," [-0.01552661  0.0235538   0.16825335  0.4927227   0.40647098 -0.1179717\n","  0.07458105  0.03931569  0.3969359   0.3678209 ]\n","Total Probability for attention weight alpha at time step 0 is 1.0 \n","-------------------------------------------------------------------------------------------------\n","Attention weights alpha for encoder time step 1: \n"," [ 0.3961097   0.5748293   0.35734907 -0.46502495 -0.2097977  -0.21193908\n"," -0.20779058  0.70661074  0.0686395   0.05519234]\n","Total Probability for attention weight alpha at time step 1 is 1.0 \n","-------------------------------------------------------------------------------------------------\n","Attention weights alpha for encoder time step 2: \n"," [-0.49231014  0.1933679   0.01701897  0.5053459   0.14219755  0.11136153\n"," -0.4771728   0.2734328  -0.48951578 -0.39408368]\n","Total Probability for attention weight alpha at time step 2 is 1.0 \n","-------------------------------------------------------------------------------------------------\n","Attention weights alpha for encoder time step 3: \n"," [-0.28182104  0.051457   -0.18036439  0.14110309 -0.3253733  -0.42577553\n","  0.0709542   0.3340743   0.00675076 -0.11887902]\n","Total Probability for attention weight alpha at time step 3 is 1.0 \n","-------------------------------------------------------------------------------------------------\n","Attention weights alpha for encoder time step 4: \n"," [-0.221969   -0.4378495  -0.30537242 -0.12181492 -0.45615157  0.03533393\n"," -0.08863887 -0.06407497 -0.01198766 -0.525242  ]\n","Total Probability for attention weight alpha at time step 4 is 1.0 \n","-------------------------------------------------------------------------------------------------\n","Final alpha is :\n","<tf.Variable 'Variable:0' shape=(10, 5) dtype=float32, numpy=\n","array([[-0.01552661,  0.3961097 , -0.49231014, -0.28182104, -0.221969  ],\n","       [ 0.0235538 ,  0.5748293 ,  0.1933679 ,  0.051457  , -0.4378495 ],\n","       [ 0.16825335,  0.35734907,  0.01701897, -0.18036439, -0.30537242],\n","       [ 0.4927227 , -0.46502495,  0.5053459 ,  0.14110309, -0.12181492],\n","       [ 0.40647098, -0.2097977 ,  0.14219755, -0.3253733 , -0.45615157],\n","       [-0.1179717 , -0.21193908,  0.11136153, -0.42577553,  0.03533393],\n","       [ 0.07458105, -0.20779058, -0.4771728 ,  0.0709542 , -0.08863887],\n","       [ 0.03931569,  0.70661074,  0.2734328 ,  0.3340743 , -0.06407497],\n","       [ 0.3969359 ,  0.0686395 , -0.48951578,  0.00675076, -0.01198766],\n","       [ 0.3678209 ,  0.05519234, -0.39408368, -0.11887902, -0.525242  ]],\n","      dtype=float32)>\n"],"name":"stdout"}]}]}