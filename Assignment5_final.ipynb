{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment5_final.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1UqM3IW4GLFbvTKELDkkRUINIIEWlXSNK","authorship_tag":"ABX9TyOitvljbzZeLZKp+z0mv9cq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PnLZ8ATJybip","colab_type":"text"},"source":["# Assignment 5 submission: Language Modeling & Recurrent Neural Networks\n","\n","## Group members: Divya Sasidharan, Poornima Venkatesha, Sinchana Eshwarappa Prameela"]},{"cell_type":"code","metadata":{"id":"pS24EsXIa5X5","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrkas6zZnMqW","colab_type":"code","outputId":"f552adb1-2af5-4fd4-af10-f5b5f62e456b","executionInfo":{"status":"ok","timestamp":1590353775386,"user_tz":-120,"elapsed":2689,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive; drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tYSHLcHfqFbj","colab_type":"code","outputId":"c9e81010-3ecd-4599-bdd1-09176fcb4005","executionInfo":{"status":"ok","timestamp":1590353775387,"user_tz":-120,"elapsed":2675,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mpcWjQ8An9JN","colab_type":"code","outputId":"6b26c3cb-8931-4844-8f99-64621fbb5128","executionInfo":{"status":"ok","timestamp":1590353775388,"user_tz":-120,"elapsed":2656,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","os.getcwd()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"r_-LARkco5Lr","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gOYNIblvx4l","colab_type":"code","outputId":"ebfe5909-f1e3-4204-8cb8-7061673ba31b","executionInfo":{"status":"ok","timestamp":1590353783354,"user_tz":-120,"elapsed":10563,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["!pip install ipython-autotime\n","%load_ext autotime"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting ipython-autotime\n","  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n","Building wheels for collected packages: ipython-autotime\n","  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=fb4451f36bf16f88d77268139417d86e89c43506e7425786c9842f1d01d4fdbc\n","  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n","Successfully built ipython-autotime\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yYVg3Ljvzp9X","colab_type":"code","outputId":"d844c83c-45ba-46d2-aa9a-d7fa401a20c9","executionInfo":{"status":"ok","timestamp":1590353788947,"user_tz":-120,"elapsed":16149,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python prepare_data.py shakespeare_input.txt skp"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-05-24 20:56:25.836642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","Split input into 22981 sequences...\n","Serialized 100 sequences...\n","Serialized 200 sequences...\n","Serialized 300 sequences...\n","Serialized 400 sequences...\n","Serialized 500 sequences...\n","Serialized 600 sequences...\n","Serialized 700 sequences...\n","Serialized 800 sequences...\n","Serialized 900 sequences...\n","Serialized 1000 sequences...\n","Serialized 1100 sequences...\n","Serialized 1200 sequences...\n","Serialized 1300 sequences...\n","Serialized 1400 sequences...\n","Serialized 1500 sequences...\n","Serialized 1600 sequences...\n","Serialized 1700 sequences...\n","Serialized 1800 sequences...\n","Serialized 1900 sequences...\n","Serialized 2000 sequences...\n","Serialized 2100 sequences...\n","Serialized 2200 sequences...\n","Serialized 2300 sequences...\n","Serialized 2400 sequences...\n","Serialized 2500 sequences...\n","Serialized 2600 sequences...\n","Serialized 2700 sequences...\n","Serialized 2800 sequences...\n","Serialized 2900 sequences...\n","Serialized 3000 sequences...\n","Serialized 3100 sequences...\n","Serialized 3200 sequences...\n","Serialized 3300 sequences...\n","Serialized 3400 sequences...\n","Serialized 3500 sequences...\n","Serialized 3600 sequences...\n","Serialized 3700 sequences...\n","Serialized 3800 sequences...\n","Serialized 3900 sequences...\n","Serialized 4000 sequences...\n","Serialized 4100 sequences...\n","Serialized 4200 sequences...\n","Serialized 4300 sequences...\n","Serialized 4400 sequences...\n","Serialized 4500 sequences...\n","Serialized 4600 sequences...\n","Serialized 4700 sequences...\n","Serialized 4800 sequences...\n","Serialized 4900 sequences...\n","Serialized 5000 sequences...\n","Serialized 5100 sequences...\n","Serialized 5200 sequences...\n","Serialized 5300 sequences...\n","Serialized 5400 sequences...\n","Serialized 5500 sequences...\n","Serialized 5600 sequences...\n","Serialized 5700 sequences...\n","Serialized 5800 sequences...\n","Serialized 5900 sequences...\n","Serialized 6000 sequences...\n","Serialized 6100 sequences...\n","Serialized 6200 sequences...\n","Serialized 6300 sequences...\n","Serialized 6400 sequences...\n","Serialized 6500 sequences...\n","Serialized 6600 sequences...\n","Serialized 6700 sequences...\n","Serialized 6800 sequences...\n","Serialized 6900 sequences...\n","Serialized 7000 sequences...\n","Serialized 7100 sequences...\n","Serialized 7200 sequences...\n","Serialized 7300 sequences...\n","Serialized 7400 sequences...\n","Serialized 7500 sequences...\n","Serialized 7600 sequences...\n","Serialized 7700 sequences...\n","Serialized 7800 sequences...\n","Serialized 7900 sequences...\n","Serialized 8000 sequences...\n","Serialized 8100 sequences...\n","Serialized 8200 sequences...\n","Serialized 8300 sequences...\n","Serialized 8400 sequences...\n","Serialized 8500 sequences...\n","Serialized 8600 sequences...\n","Serialized 8700 sequences...\n","Serialized 8800 sequences...\n","Serialized 8900 sequences...\n","Serialized 9000 sequences...\n","Serialized 9100 sequences...\n","Serialized 9200 sequences...\n","Serialized 9300 sequences...\n","Serialized 9400 sequences...\n","Serialized 9500 sequences...\n","Serialized 9600 sequences...\n","Serialized 9700 sequences...\n","Serialized 9800 sequences...\n","Serialized 9900 sequences...\n","Serialized 10000 sequences...\n","Serialized 10100 sequences...\n","Serialized 10200 sequences...\n","Serialized 10300 sequences...\n","Serialized 10400 sequences...\n","Serialized 10500 sequences...\n","Serialized 10600 sequences...\n","Serialized 10700 sequences...\n","Serialized 10800 sequences...\n","Serialized 10900 sequences...\n","Serialized 11000 sequences...\n","Serialized 11100 sequences...\n","Serialized 11200 sequences...\n","Serialized 11300 sequences...\n","Serialized 11400 sequences...\n","Serialized 11500 sequences...\n","Serialized 11600 sequences...\n","Serialized 11700 sequences...\n","Serialized 11800 sequences...\n","Serialized 11900 sequences...\n","Serialized 12000 sequences...\n","Serialized 12100 sequences...\n","Serialized 12200 sequences...\n","Serialized 12300 sequences...\n","Serialized 12400 sequences...\n","Serialized 12500 sequences...\n","Serialized 12600 sequences...\n","Serialized 12700 sequences...\n","Serialized 12800 sequences...\n","Serialized 12900 sequences...\n","Serialized 13000 sequences...\n","Serialized 13100 sequences...\n","Serialized 13200 sequences...\n","Serialized 13300 sequences...\n","Serialized 13400 sequences...\n","Serialized 13500 sequences...\n","Serialized 13600 sequences...\n","Serialized 13700 sequences...\n","Serialized 13800 sequences...\n","Serialized 13900 sequences...\n","Serialized 14000 sequences...\n","Serialized 14100 sequences...\n","Serialized 14200 sequences...\n","Serialized 14300 sequences...\n","Serialized 14400 sequences...\n","Serialized 14500 sequences...\n","Serialized 14600 sequences...\n","Serialized 14700 sequences...\n","Serialized 14800 sequences...\n","Serialized 14900 sequences...\n","Serialized 15000 sequences...\n","Serialized 15100 sequences...\n","Serialized 15200 sequences...\n","Serialized 15300 sequences...\n","Serialized 15400 sequences...\n","Serialized 15500 sequences...\n","Serialized 15600 sequences...\n","Serialized 15700 sequences...\n","Serialized 15800 sequences...\n","Serialized 15900 sequences...\n","Serialized 16000 sequences...\n","Serialized 16100 sequences...\n","Serialized 16200 sequences...\n","Serialized 16300 sequences...\n","Serialized 16400 sequences...\n","Serialized 16500 sequences...\n","Serialized 16600 sequences...\n","Serialized 16700 sequences...\n","Serialized 16800 sequences...\n","Serialized 16900 sequences...\n","Serialized 17000 sequences...\n","Serialized 17100 sequences...\n","Serialized 17200 sequences...\n","Serialized 17300 sequences...\n","Serialized 17400 sequences...\n","Serialized 17500 sequences...\n","Serialized 17600 sequences...\n","Serialized 17700 sequences...\n","Serialized 17800 sequences...\n","Serialized 17900 sequences...\n","Serialized 18000 sequences...\n","Serialized 18100 sequences...\n","Serialized 18200 sequences...\n","Serialized 18300 sequences...\n","Serialized 18400 sequences...\n","Serialized 18500 sequences...\n","Serialized 18600 sequences...\n","Serialized 18700 sequences...\n","Serialized 18800 sequences...\n","Serialized 18900 sequences...\n","Serialized 19000 sequences...\n","Serialized 19100 sequences...\n","Serialized 19200 sequences...\n","Serialized 19300 sequences...\n","Serialized 19400 sequences...\n","Serialized 19500 sequences...\n","Serialized 19600 sequences...\n","Serialized 19700 sequences...\n","Serialized 19800 sequences...\n","Serialized 19900 sequences...\n","Serialized 20000 sequences...\n","Serialized 20100 sequences...\n","Serialized 20200 sequences...\n","Serialized 20300 sequences...\n","Serialized 20400 sequences...\n","Serialized 20500 sequences...\n","Serialized 20600 sequences...\n","Serialized 20700 sequences...\n","Serialized 20800 sequences...\n","Serialized 20900 sequences...\n","Serialized 21000 sequences...\n","Serialized 21100 sequences...\n","Serialized 21200 sequences...\n","Serialized 21300 sequences...\n","Serialized 21400 sequences...\n","Serialized 21500 sequences...\n","Serialized 21600 sequences...\n","Serialized 21700 sequences...\n","Serialized 21800 sequences...\n","Serialized 21900 sequences...\n","Serialized 22000 sequences...\n","Serialized 22100 sequences...\n","Serialized 22200 sequences...\n","Serialized 22300 sequences...\n","Serialized 22400 sequences...\n","Serialized 22500 sequences...\n","Serialized 22600 sequences...\n","Serialized 22700 sequences...\n","Serialized 22800 sequences...\n","Serialized 22900 sequences...\n","time: 5.83 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QGCIG-opzVYe","colab_type":"code","outputId":"54fbd1f5-98ad-4002-b68a-977f959e589b","executionInfo":{"status":"ok","timestamp":1590354682481,"user_tz":-120,"elapsed":834,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from prepare_data import parse_seq\n","import pickle\n","\n","sequence_length=200\n","\n","\n","data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n","#print(data)\n","\n","data = data.map(lambda x: parse_seq(x, sequence_length))\n","print(data)\n","BATCH_SIZE = 128\n","BUFFER_SIZE = 50000\n","data = data.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n","# a map from characters to indices\n","vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n","vocab_size = len(vocab)\n","# inverse mapping: indices to characters\n","ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n","\n","print(vocab)\n","print(vocab_size)\n","print(ind_to_ch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<MapDataset shapes: (200,), types: tf.int32>\n","{'\\n': 1, 'u': 2, 'a': 3, 'K': 4, '3': 5, 'C': 6, 'y': 7, 'V': 8, 'U': 9, 'b': 10, '!': 11, 'g': 12, 'B': 13, 'd': 14, 'w': 15, 'n': 16, 'E': 17, 'P': 18, '-': 19, ';': 20, 'D': 21, 'J': 22, 'k': 23, 't': 24, 'G': 25, 'o': 26, 'c': 27, 'm': 28, 'X': 29, 'M': 30, 'v': 31, 'R': 32, 'h': 33, 'r': 34, 'l': 35, 'O': 36, 'x': 37, 'S': 38, 'N': 39, 'T': 40, ' ': 41, '.': 42, \"'\": 43, 'Z': 44, '?': 45, 'p': 46, '&': 47, '$': 48, ']': 49, 'A': 50, 'Q': 51, ':': 52, 's': 53, 'Y': 54, 'H': 55, 'f': 56, 'L': 57, ',': 58, 'q': 59, 'e': 60, 'F': 61, 'I': 62, '[': 63, 'i': 64, 'W': 65, 'z': 66, 'j': 67, '<S>': 0}\n","68\n","{1: '\\n', 2: 'u', 3: 'a', 4: 'K', 5: '3', 6: 'C', 7: 'y', 8: 'V', 9: 'U', 10: 'b', 11: '!', 12: 'g', 13: 'B', 14: 'd', 15: 'w', 16: 'n', 17: 'E', 18: 'P', 19: '-', 20: ';', 21: 'D', 22: 'J', 23: 'k', 24: 't', 25: 'G', 26: 'o', 27: 'c', 28: 'm', 29: 'X', 30: 'M', 31: 'v', 32: 'R', 33: 'h', 34: 'r', 35: 'l', 36: 'O', 37: 'x', 38: 'S', 39: 'N', 40: 'T', 41: ' ', 42: '.', 43: \"'\", 44: 'Z', 45: '?', 46: 'p', 47: '&', 48: '$', 49: ']', 50: 'A', 51: 'Q', 52: ':', 53: 's', 54: 'Y', 55: 'H', 56: 'f', 57: 'L', 58: ',', 59: 'q', 60: 'e', 61: 'F', 62: 'I', 63: '[', 64: 'i', 65: 'W', 66: 'z', 67: 'j', 0: '<S>'}\n","time: 53.8 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F4Zlo8g68Fsf","colab_type":"code","outputId":"4894752f-7b59-4008-addd-5ee425b592a3","executionInfo":{"status":"ok","timestamp":1590359996962,"user_tz":-120,"elapsed":1680,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# view data\n","print(data)\n","for index ,value in enumerate(data): \n","  print(index) \n","  print(value.shape)\n","  print(\"----example sentence in index-----\")\n","  print(value[0])\n","  vector=tf.one_hot(value,vocab_size)  \n","  s=[]\n","  sentence=\"\"\n","  for i in value[0].numpy():     \n","    s.append(ind_to_ch[i])\n","  print(\"----example sentence-----\")\n","  print(sentence.join(s))\n","  print(\"----vector representation-----\")\n","  print(vector)\n","  break\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["<RepeatDataset shapes: (None, 200), types: tf.int32>\n","0\n","(128, 200)\n","----example sentence in index-----\n","tf.Tensor(\n","[ 0 57 38 40 50 61 61 52  1 30 64 53 24 34 60 53 53 41 61 26 34 14 58 41\n"," 10  7 41 28  7 41 24 34 26 24 33 58 41  7 26  2 41  3 34 60 41 31 60 34\n","  7 41 15 60 35 35 41 28 60 24 52  1 10  7 41  7 26  2 34 41 35 60  3 31\n"," 60 58 41 12 26 26 14 41 28 64 53 24 34 60 53 53 42  1  1 18 50 25 17 52\n","  1 65 64 56 60 58 41 10 64 14 41 24 33 60 53 60 41 12 60 16 24 35 60 28\n"," 60 16 41 15 60 35 27 26 28 60 42 41  6 26 28 60 58 41 15 60 41 33  3 31\n"," 60 41  3  1 33 26 24 41 31 60 16 64 53 26 16 41 46  3 53 24  7 41 24 26\n"," 41 14 64 16 16 60 34 52 41 27 26 28 60 58 41 12 60 16 24 35 60 28 60 16\n"," 58 41 62 41 33 26 46 60], shape=(200,), dtype=int32)\n","----example sentence-----\n","<S>LSTAFF:\n","Mistress Ford, by my troth, you are very well met:\n","by your leave, good mistress.\n","\n","PAGE:\n","Wife, bid these gentlemen welcome. Come, we have a\n","hot venison pasty to dinner: come, gentlemen, I hope\n","----vector representation-----\n","tf.Tensor(\n","[[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 1. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 1. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]]], shape=(128, 200, 68), dtype=float32)\n","time: 839 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o8ZwrlwXHD8X","colab_type":"code","outputId":"bbb0ff27-8e6e-4abf-ee7c-a7e05769673a","executionInfo":{"status":"ok","timestamp":1590354939410,"user_tz":-120,"elapsed":799,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":748}},"source":["init=tf.keras.initializers.GlorotUniform()\n","x=tf.Variable(init([68,512]))\n","y=tf.Variable(init([512,512]))\n","\n","print(x)\n","print(y)\n","print(tf.matmul(x,y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<tf.Variable 'Variable:0' shape=(68, 512) dtype=float32, numpy=\n","array([[ 0.00993611, -0.02615932,  0.02099883, ...,  0.01232596,\n","        -0.02640367,  0.04674964],\n","       [ 0.04844298,  0.09384456, -0.02246771, ...,  0.00501551,\n","         0.07491494,  0.0732724 ],\n","       [-0.0874153 , -0.04493261,  0.07449607, ..., -0.08990824,\n","        -0.03436653, -0.03187849],\n","       ...,\n","       [ 0.08851036,  0.05935428, -0.00863193, ...,  0.07777705,\n","         0.03478342,  0.03632151],\n","       [ 0.06806874, -0.08718722,  0.03238624, ...,  0.0445128 ,\n","         0.00836727,  0.00430316],\n","       [-0.00315545,  0.00741589,  0.06568872, ...,  0.03165784,\n","         0.07613265, -0.02535073]], dtype=float32)>\n","<tf.Variable 'Variable:0' shape=(512, 512) dtype=float32, numpy=\n","array([[ 0.0664966 ,  0.05369394,  0.07639772, ...,  0.00291781,\n","         0.0308578 ,  0.0077654 ],\n","       [-0.02979588,  0.0265544 , -0.06521568, ..., -0.03492369,\n","        -0.05083403,  0.05136079],\n","       [ 0.00974547,  0.04228942,  0.01687609, ..., -0.02900057,\n","        -0.00048078, -0.06747656],\n","       ...,\n","       [ 0.07040704,  0.03712224,  0.03279213, ...,  0.06924652,\n","        -0.03983074, -0.05980063],\n","       [ 0.00493686,  0.06571025,  0.01130197, ...,  0.02962438,\n","         0.03714076,  0.03855953],\n","       [-0.01089116, -0.0703035 ,  0.04848006, ..., -0.00014429,\n","         0.03507558,  0.05853093]], dtype=float32)>\n","tf.Tensor(\n","[[-0.05389012 -0.00369502  0.01602153 ...  0.0205848   0.0182835\n","  -0.07268604]\n"," [ 0.06418782 -0.02832666  0.06991446 ... -0.05338085 -0.00449002\n","  -0.03980102]\n"," [ 0.01087093  0.0900945   0.04901166 ...  0.01531461  0.00531365\n","  -0.02604181]\n"," ...\n"," [ 0.08475824 -0.03917069 -0.06197793 ...  0.11973886 -0.01210594\n","   0.02257226]\n"," [ 0.00045129 -0.02851701 -0.03905585 ...  0.03965484  0.05886763\n","  -0.02027031]\n"," [ 0.05162391 -0.0267795   0.00329787 ... -0.14751297  0.03723127\n","  -0.05922557]], shape=(68, 512), dtype=float32)\n","time: 24.3 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JE8JyzS0vEX1","colab_type":"code","outputId":"de78a247-8627-475a-edc9-a229a4522c39","executionInfo":{"status":"ok","timestamp":1590357816660,"user_tz":-120,"elapsed":1219,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#parameter initialisation              \n","hidden_dim = 512         \n","\"\"\"\n","U is the weight matrix for weights between input and hidden layers\n","W is the weight matrix for shared weights in the RNN layer (hidden layer)\n","V is the weight matrix for weights between hidden and output layers\n","\"\"\"\n","init=tf.keras.initializers.GlorotUniform()\n","U=tf.Variable(init([vocab_size,hidden_dim]))\n","W=tf.Variable(init([hidden_dim,hidden_dim]))\n","b_W=tf.Variable(tf.zeros([hidden_dim]))\n","\n","V=tf.Variable(init([hidden_dim,vocab_size]))\n","b_V=tf.Variable(tf.zeros([vocab_size]))\n","\n","parameters=[U,W,b_W,V,b_V]\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# start training using Gradient Tape\n","@tf.function\n","def train_step(input):\n","  length=input.shape[1]-1 #length is 199 after removing <s>\n","  with tf.GradientTape() as tape:\n","    # During forward propagation we save all hidden states in h because need them later.\n","    # We add one additional element for the initial hidden, which we set to 0     \n","    h=tf.zeros([input.shape[0],hidden_dim])    \n","    loss=tf.TensorArray(tf.float32,size=length )            \n","    for t in tf.range(length):\n","      current_char=input[:,t]\n","      next_char=input[:,t+1]\n","      vector=tf.one_hot(current_char,vocab_size)      \n","      a=tf.matmul(h,W)+tf.matmul(vector,U)+b_W\n","      h=tf.nn.tanh(a)      \n","      c=tf.matmul(h,V)+b_V      \n","      #o=tf.nn.softmax(c) \n","      loss_t=tf.nn.sparse_softmax_cross_entropy_with_logits(next_char,c)      \n","      #loss=loss+loss_t \n","      loss=loss.write(t,loss_t)      \n","    #loss=loss/(tf.cast(length,tf.float32)) \n","    loss=loss.stack()    \n","    final_loss=tf.reduce_mean(loss)    \n","  gradients = tape.gradient(final_loss, parameters)\n","  grads, _ = tf.clip_by_global_norm(gradients,1)# gradient clipping  \n","  optimizer.apply_gradients(zip(grads, parameters))  \n","  return final_loss"],"execution_count":0,"outputs":[{"output_type":"stream","text":["time: 54.9 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ufRIQkUdR9nt","colab_type":"code","outputId":"b866b83e-9d5f-4f31-a7d6-56467dfd3ad0","executionInfo":{"status":"ok","timestamp":1590358623284,"user_tz":-120,"elapsed":804082,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["steps = 3000\n","#start of training\n","\n","for index ,value in enumerate(data):    \n","  loss=train_step(value)    \n","  if not index % 500 :   \n","    print(index, loss)\n","        \n","  if index >= steps:\n","    break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 tf.Tensor(4.236715, shape=(), dtype=float32)\n","500 tf.Tensor(2.0259883, shape=(), dtype=float32)\n","1000 tf.Tensor(1.7993593, shape=(), dtype=float32)\n","1500 tf.Tensor(1.6732452, shape=(), dtype=float32)\n","2000 tf.Tensor(1.6013917, shape=(), dtype=float32)\n","2500 tf.Tensor(1.5523757, shape=(), dtype=float32)\n","3000 tf.Tensor(1.4987805, shape=(), dtype=float32)\n","time: 13min 23s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U5Ge4xoZyq53","colab_type":"code","outputId":"85c4192d-718a-4fc5-8485-4fa24bf1fdd7","executionInfo":{"status":"ok","timestamp":1590358815561,"user_tz":-120,"elapsed":3885,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["start = 'l'\n","start_index = [vocab[start]]\n","print(start_index)\n","h_t=tf.zeros([1,hidden_dim])\n","for i in range(2000):\n","  h_t=tf.nn.tanh(tf.matmul(tf.one_hot(start_index[-1:], depth = vocab_size), U) + tf.matmul(h_t, W) + b_W)#each iteration take the last char\n","  o_t=tf.nn.softmax(tf.matmul(h_t,V)+b_V).numpy()[0]\n","  start_index.append(np.random.choice(vocab_size, p = o_t))\n","text=\"\".join([ind_to_ch[i] for i in start_index])\n","print(text)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[35]\n","lifb?\n","Is told this knightly which of may say!\n","Teed loat of life me beak your hours for for,\n","As the sexty shall were wish theas it that\n","drunches hath her hand: come tewors it.\n","\n","BYoS:\n","My goodngs, you wartints, thene of prood so\n","fromath of thee, that his it, if it what speak these\n","The devell are alme bying us lards in overtanen\n","To husband dream his great I think, I lises:\n","Or the matter, this is the love.\n","Beloved your soulp: at leads her as here?\n","\n","IAGO:\n","Why, then the nightyes on a shall I well,\n","That brow somethurs; the rangle abort\n","With we here their reamon see her. Be with the woild:\n","For mine own his father: were his feer,\n","Let us on our perfeats with her concerishal's coupted!\n","Buck'd. I mesir, 'tis born, I lade thee any toound,\n","To would therefore here in the epitior.\n","But that omblice, to thine tridune dalkesion lays\n","Theigh abood, and my lord of Godging of her;\n","And heremin's beg's in pleise it uidest doge,\n","The youtorm of their lording core. This griefs again?\n","\n","FALSTAFF:\n","The peoplous sticio, who can are to hell been\n","Bettien to say to be goda; but begenty,\n","To love leavesing; see, to tend you will not.\n","\n","SALIS:\n","Ay, with your royrow'd, ant thou wert made:\n","By shall turn thy cranners, as in I doubted here?\n","\n","VILEN:\n","No, fair bad mind eyez, which cease your jowr.\n","Cul that he so matter: and whence we alvest;\n","Mest comous for me.\n","\n","BIANDA:\n","We at for their sperch exturain; wearer, consire,\n","My lord with your more to bucking-stit.\n","\n","TAMOL:\n","Come where, my lord, if this with the laty,\n","As this nets wonlding to which is an\n","happty father meath.\n","\n","PISANIO:\n","O here,\n","And thanks to be the itseriousoe.\n","I hence coll.\n","\n","PELTOT:\n","No slenge. O heaverows, bryended which you shall\n","May tood I peace me be readomet?\n","\n","CRLIONAS:\n","Let me are wings to be to her fales.\n","\n","SIR: I'll be thrive the joyal of his\n","goty of her upon her,\n","I frowk that toge of thinches is tow\n","Fien, if thee know thou forkifally;\n","We'll praTher ingresse's frantor, couft. Here is a wife;\n","The wool hory. Anf I had quicks are uncrea.\n","\n","Pofter:\n","If If a g\n","time: 3.05 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JDOBbPdm3_BO","colab_type":"code","outputId":"c61a48d8-6740-40ac-977d-4bfbcce92d12","executionInfo":{"status":"ok","timestamp":1590359588378,"user_tz":-120,"elapsed":1417,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["s=tf.zeros([1,hidden_dim])\n","gen_text=[35] # start character is \"l\"\n","ouput_sentence=\"\"\n","for i in range(500):\n","  next_input = tf.one_hot(gen_text[-1:],depth=vocab_size)\n","  a_t=tf.matmul(next_input,U)+tf.matmul(s,W)+b_W\n","  s=tf.nn.tanh(a_t)      \n","  c_t=tf.matmul(s,V)+b_V      \n","  o_t=tf.nn.softmax(c_t).numpy()[0]\n","  preds =np.random.choice(vocab_size,p=o_t)\n","  gen_text.append(preds)\n","ouput_sentence=\"\".join([ind_to_ch[index] for index in gen_text])\n","print(ouput_sentence)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["lo.\n","\n","KENT:\n","What you can laid, hell to here a gracious.\n","\n","BAPTISTO:\n","My should\n","we part, as thy paperterrest dode.\n","\n","NERLON:\n","Become lockshil: there are shilla? keep these stus\n","Abswerk offines is thee all me?\n","\n","BORANAMLCH:\n","Why, follow! thind I tous it,\n","Till the would leave a shapping.\n","\n","BIRON:\n","RGial, seek, sit soet!\n","Their stillits to his pronstert.\n","\n","Second Lendse:\n","Mest gentle esent; forswer'd me for swear-lehs,\n","Your gone and friendst lee him wited me!\n","\n","DO PEMy:\n","Nawe, I do not the rive of now I hate not,\n","\n","time: 756 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uojql6MD4oQc","colab_type":"code","outputId":"0f501ac7-ffe1-43f5-af6b-52796e9eee00","executionInfo":{"status":"ok","timestamp":1590354349653,"user_tz":-120,"elapsed":576730,"user":{"displayName":"Divya Sasidharan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNKKyZIOwTaSb2KdCUIzxieTY4-XIH0Ltl6YgRFIU=s64","userId":"04337004093201313738"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["start = 3\n","limit = 18\n","delta = 3\n","for i in range(start, limit, delta):\n","  print(i)\n","\n","\n","for j in tf.range(start, limit, delta):\n","  print(j)\n","\n","\"\"\"\n","TensorFlow will only capture for loops that iterate over a tensor or a Dataset. So\n","make sure you use for i in tf.range(10) rather than for i in range(10), or\n","else the loop will not be captured in the graph. Instead, it will run during tracing.\n","This may be what you want, if the for loop is meant to build the graph, for example\n","to create each layer in a neural network.\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["3\n","6\n","9\n","12\n","15\n","tf.Tensor(3, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(9, shape=(), dtype=int32)\n","tf.Tensor(12, shape=(), dtype=int32)\n","tf.Tensor(15, shape=(), dtype=int32)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nTensorFlow will only capture for loops that iterate over a tensor or a Dataset. So\\nmake sure you use for i in tf.range(10) rather than for i in range(10), or\\nelse the loop will not be captured in the graph. Instead, it will run during tracing.\\nThis may be what you want, if the for loop is meant to build the graph, for example\\nto create each layer in a neural network.\\n'"]},"metadata":{"tags":[]},"execution_count":18},{"output_type":"stream","text":["time: 21.8 ms\n"],"name":"stdout"}]}]}